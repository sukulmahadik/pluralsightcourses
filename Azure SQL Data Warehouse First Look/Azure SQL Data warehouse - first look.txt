Azure SQL Data Warehouse: First Look
by Warner Chaves

Azure SQL Data Warehouse is Microsoft Azure's Database as a Service offering. This course teaches the differences between this service and SQL Server, designing your database for the service, loading and migrating data, scaling and basic monitoring.

Resume Course

Bookmark
Add to Channel
Download Course
Table of contents
Description
Transcript
Exercise files
Discussion
Learning Check
Related Courses
Course Overview
Course Overview
Hello everyone, my name is Warner Chaves, and welcome to my course, Azure SQL Data Warehouse: First Look. I'm a Microsoft Data Platform MVP, and a principal consultant at Pythion. Data Warehousing projects have a reputation of being very expensive to start and very expensive to operate on an ongoing basis, however, the cloud has now come to change all that by providing a data warehouse that's a fully-managed service and making it easy to provision, load, and query your data warehouse cost-effectively, and with good performance. With a full T-SQL interface and compatibility with the rest of the Microsoft Data Stack, Azure SQL Data Warehouse can fit transparently into your business data strategy, and leverage already existent and familiar development and management skills. Some of the major topics that we're going to cover include differences with the classic SQL Server, the sign-in for Azure SQL Data Warehouse, methods for loading and migrating data, and workload management and scaling the system up or down. But by the end of the course, you'll have the information necessary to jump into the service for a new data warehousing project or a migration to the cloud. Before beginning the course, you only need to be familiar with basic concepts of data warehousing and SQL Server administration. I hope you'll join me on this journey to learn data warehousing in the cloud with Azure SQL Data Warehouse: First Look course at Pluralsight.

Warehousing in the Cloud
What's in This Module?
Hi. This is Warner Chaves with Pluralsight, and welcome to my course, Azure SQL Data Warehouse: First Look. This course is an introduction to the main concepts of SQL Data Warehouse; provisioning, designing, and maintaining the system. This is module 1, Warehousing in the Cloud. In this module, we're going to go over the main concepts of Azure SQL Data Warehouse that you need to understand in order to jump in and start using the service. We're also going to go through the biggest advantages of doing warehousing in the cloud, especially as opposed to doing traditional on-premises data warehousing. Finally, we're going to go into how the service is built, and how you can provision your data warehouse in the Microsoft Azure cloud environment.

What Is Azure SQL Data Warehouse?
So first, what is Azure SQL Data Warehouse? It's a service that lives on Microsoft Azure, Microsoft's offering of public cloud data centers. It's also a Platform as a Service offering, also known as Database as a Service. That means that you don't have to manage the operating system on anything _____ that. The only thing you have to worry about is about loading and querying your database. It's also a massively parallel processing system, so as opposed to the usual traditional warehousing where it's one big machine, this is actually a distributed system where different computers, called nodes, cooperate to give you the answers to your queries. Because of the way Azure SQL Data Warehouse works, where it's split into multiple machines, it depends on distributed storage. And like we just said, because it uses multiple machines to compute the answers to your queries, it is also also distributed compute. So an SMP system, or Symmetric Multiprocessing system will be more like the classical SQL Server that you're probably familiar with, in that usually, that is one large machine, either on-premises on the client, physical or virtualized. An MPP system, on the other hand, is a massively parallel processing system. These types of systems are composed of multiple machines, not just one big machine. Usually these machines will all have a slice of the data from the database, and when a query comes in there is a coordination done of query execution planning so that there's distributed work that happens on each machine in order to get you the answer to your queries. This scales very well as the amount of data increases. In Azure SQL Data Warehouse, we have a concept called the Data Warehousing Unit. This is a measure of the underlying compute power of the database. What this means is that on Azure SQL Data Warehouse, you don't have to select specific configurations of CPUs, RAM or storage. The only thing you have to worry about is the amount of Data Warehousing Units that you provision. Let's check out an example. Let's say I provision a data warehouse with 100 DW units. I did a test, and then I loaded three tables in 15 minutes, I ran a report, and then my report came back in 20 minutes. I then adjusted my data warehouse and increased my compute power to 500 Data Warehousing Units. When I ran my test again, this time I loaded three tables in 3 minutes, and I ran my report in 4 minutes. Basically, the Data Warehousing Unit is a comparison unit. If I do a workload in 100 DWU and then I increase that 5 fold to 500 DWU, I should expect an approximate 5 times improvement on my workload.

Cloud Versus Traditional Warehouse
So why choose the cloud over on-premises traditional warehousing? There are several good reasons. First, in the cloud you don't need to do large capital expenses to get started. On-premises, it can be really hard to get all the approvals and justify the big expense of buying large warehousing machines and licensing. In the cloud, you don't need a lot of staff to maintain your hardware solution, the virtualization, or the operating system. Like I mentioned before, Azure SQL Data Warehouse is a Platform as a Service offering,, so all of these things are taken care for you. All you need to do is manage the database itself. Finally, something that is very difficult to do on-premises is to scale storage and compute up or down on demand. Usually you do a big investment, and you can't scale it down at all. Or, if you want to scale up, this usually takes time to provision and set up the extra compute power. These operations, on the other hand, can happen transparently, and in minutes, in the cloud. So how is Azure SQL Data Warehouse billed? First of all, there's a component of storage that you have to pay, and it is billed by the gigabyte, usually monthly. You have two choices of storage, there's standard or premium geo redundant storage. Right now it depends on the region where you are creating your data warehouse whether you will get standard or you will get premium storage. There is no cost for storage transactions. This is different from the standard way in which blob storage works. Sometimes you can be charged depending on a large amount of storage transactions. This is now how it works with Azure SQL Data Warehouse. You only pay for the amount of storage, not for the transactions itself. Finally, outbound data transferred is billed as well. So if you are loading a lot of data into your data warehouse, you will not get charges for that, but if you are also taking out a lot of data out of your data warehouse, and out of the Azure data center, that will have an outbound data transfer charge. The other piece of how the service is billed is the compute power, like we mentioned, the Data Warehousing Unit. So the amount of Data Warehousing Units can go from 100, which is the minimum, to 2000 data warehousing units in one database. That means you can go from a base performance to about 20 times that base performance. Data Warehousing Units are built per hour. So if, for example, you increase from 100 Data Warehousing Units to 500, you will get billed that hour for 500 Data Warehousing Units. If on the next hour you bring it back down, then again, you will go back to paying 100 Data Warehousing Units. However, if 1 hour has two different values of warehousing units, you will pay for the highest one during that hour. When not in use, compute power of the Azure SQL Data Warehouse, can actually be completely paused for maximum savings. So if, for example, your data warehouse gets no use during the weekends, you can pause it completely and save yourself that cost. While the fundamentals and best practices of the service will remain the same, in the cloud it is common for service providers to change details like the service limits or the pricing. In order to get the latest info, refer to azure. com. Let me show you where. I'm on the Home page right now for www. azure. com. Here, you can just go to pricing. On the pricing page, scroll down, and you'll get a list of all the services. Azure SQL Data Warehouse is located under Data + Storage. We'll just click there. In this page, we can get the details and the pricing for SQL Data Warehouse. We can get the limits on compute, like the 100 DWU lower limit and the price for it, as well as the current maximum DWU limit, 2000, and the price for it. If we keep scrolling down, we'll also get the limits on the storage, and currently which regions have premium storage, and which regions have standard storage at the moment. For the documentation, it's the same. We can click on Documentation, and then go to Data + Storage, and then here go to SQL Data Warehouse, and we can access the full home page of documentation for the service.

Provisioning the Data Warehouse
Let's look at the process of provisioning the data warehouse. First, we need to select a region. If we already have Azure services or Azure machines that we know are going to be sources of data for the data warehouse, then that region is probably a good choice. If not, and you have on-premises sources that you want to move their data to Azure Data Warehouse, you want to select a region that is closest to you. Second, you can select an existing SQL Server that you have on Azure Data Warehouse, or you can create a new one when you provision the data warehouse in the portal. Once you have selected the server where your data warehouse is going to live, you can pick the origin of the data, by that I mean that you can load a sample warehouse that Microsoft makes available to you just to play with the service, you can start with a blank database, or if you already have an existing data warehouse, you can create a new one based on the backup of the existing one. Finally, you have to select your DWU level. Remember, this is the compute power that you want for your data warehouse. You can start with a small amount of Data Warehousing Units, 100 is the minimum, and then slowly scale up until you have the performance that you want. There are three methods of provisioning your Azure SQL Data Warehouse. First, you can do it in the portal by going into New, and then the Data + Storage option. Second, you can use PowerShell and the Azure cmdlets, using the new AzureRmSqlDatabase cmdlet and specifying data warehouse as the addition. Finally, you can do it in T-SQL by connecting to your SQL Server that's already living in Azure, and then specifying a CREATE DATABASE command specifying the data warehouse as the addition, and the amount of Data Warehousing Units that you want.

Demo: Provisioning Through the Azure Web Portal
Let's check out a demo of provisioning a data warehouse through the Azure Web Portal. I'm connected now to my Azure subscription, and this is the Dashboard. I just have to go to New. Once we're at New, we look for the Data + Storage option in the blade, and once we're at Data + Storage we have the option of SQL Data Warehouse. Note that it will say that it's in preview. As of the moment of this recording, it is not on general availability yet. First thing, we just have to pick a database name. In this case I'll just name it warnerwarehouse. Next we will select either to use an existing resource group or create a new one. I'm going to create a new one called warehouses. Then we select the origin of the data, Blank, Sample database from Microsoft, or a Backup from another warehouse. I'll just leave it Blank in this case. And I'm going to create a new server as part of the demo so that you can see the entire process end-to-end. I'll just name the server warnerdatawarehouse, and then I have to set up an admin login and a secure password. After I've set up the password, it's time to pick the location of the server. And here you have to select dependent on your business needs or data sources. I'm going to select East US in this case. At this point everything is ready, I just click on Select. Now that we have the server information populated on the warehouse blade, we can pick the performance level or the Data Warehousing Units. In this case the default is 400, but I'm going to change it to 100. Once I've selected it and changed it to 100, I'll actually get an estimation from Azure of the cost per hour. In this case it's 85 Canadian dollar cents per hour. If that sounds good, then just click Create and the deployment will get submitted to the Azure fabric. Once we verify the deployment is submitted, it's just a matter of waiting until it's completed. That can take a few minutes. We'll get a notification once it has completed. In this case, we'll know the deployment has succeeded if we don't come up with any errors from the portal. We can actually go back and search for warehouses, and that'll lead us to our Azure SQL Server that we created, and we can make sure that the server was created correctly in the region that we wanted. We can also verify that it was created with one database inside, a database called warnerwarehouse, and we can see the data warehouse level there as DW100, as well. We can click and expand on the warehouse blade as well, and here we can also go into the server name so that we can try to connect and actually run a query against our data warehouse. We'll just click to copy the server name, then we can paste that server name into Maintenance Studio and use the admin account that I created when I created the server. Now if I try to connect at this point in time, I'm going to get an error, because we don't have a firewall rule to allow my virtual machine to connect to my data warehouse. So let's check how we can allow my machine in the portal as well. So let's go back to the portal. We'll go back to the Server blade so we can close that Warehouse blade, go to the Settings, and then go to the Firewall configuration setting at the server level. Once on the Firewall blade, we can add the Client IP if we're connected to the machine we want to whitelist, and save the new setting on the firewall. And we got a message that it was successfully updated, so we can try our connection again. Now we'll just try to reconnect. This time it goes through. We can verify we're connected to the warnerdatawarehouses. If we expand the databases, we can see the warnerwarehouse database has been created, and I can now run queries against it as well. So I'll select the database, go to New Query, and then just run a simple select @@version query so we can verify that we are indeed running on the Warehouse edition of SQL Server. And actually expand the result set, and we can see that the product name here is Parallel Data Warehouse, which is the name of the version of SQL Server that is the distributed MPP Data Warehouse edition.

Demo: Provisioning with Azure PowerShell
Let's see a demo of provisioning the Azure SQL Data Warehouse with Azure PowerShell. Okay, so before running PowerShell scripts, you need to install the Azure PowerShell. Just go to www. azure. com, and search for Azure PowerShell, and you'll be able to find the installation package. Once you have that going, you can set up your account first by using the Login-AzureRmAccount. That will prompt you for your credentials for your Azure subscription. After that, if you have multiple Azure subscriptions you can pick a preferred one to create the objects and use as your default. If you want to see the subscriptions that you have, you can use Get-AzureRmSubscription, and it will give you a list of your subscriptions, and then you can select your default subscription by using Get-AzureRmSubscription, passing the subscription as the parameter name SubscriptionName, and then piping that into Select-AzureRmSubscription. Once you have your login set up and you have selected your default subscription, we can move forward with actually creating the data warehouse. Let's look into that. So here's the cmdlet to create a new data warehouse. The cmdlet is New-AzureRmSqlDatabase. We have to pass in the RequestedServiceObjectiveName, or basically the amount of Data Warehousing Units that we want for our data warehouse. In this case, I am passing "DW100", so 100 Data Warehousing Units. I then pass a DatabaseName, in this case I'm just calling it "warnerwarehousePShell". I have to pass a server name, so if the server did not exist, you should have created it before that. In this case, I am using the server that I created in the previous demo, and then I am passing in the ResourceGroupName. I am also using the resource group that I created in the previous demo called "warehouses". And then finally, I'm just passing the Edition. In this case it's a "DataWarehouse", so that it instantiates as an Azure SQL Data Warehouse and not a regular SQL database. Once we have all of those pieces, we can actually run the command. After a few minutes, the data warehouse will have been created, and we can verify in the output. We can see a ResourceGroupName, the warehouses, the ServerName that we specified, the new DatabaseName, the warnerwarehousePShell, the Location, _____ East US, and all the other different configuration values for deployment, including Collation, Status, CreationDate, etc. At this point, since we didn't get any errors in the output, we can go into Management Studio where I already connected to the warnerdatawarehouse server, I can refresh the list, and as you can see, we have not only the warnerwarehouse we created with Web Portal, but we also have the new one, warnerwarehousePShell, that we created through the PowerShell.

Module Summary
In this module, we learned that Azure SQL Data Warehouse is a Platform as a Service offering inside Microsoft Azure. We learned that it offers multiple advantages over traditional on-premises warehousing, such as decrease of staff, easiness of scaling up or down the resources, and not having to do a large up-front investment. We also learned that the service is billed based on two components, the storage and the compute, and the compute can also be changed based on the Data Warehousing Unit, or how much of those units you need. And finally, we learned that there are different methods that we can use to provision the service, depending on what we want to do, either one off through the web portal, or easily to code it through T-SQL or PowerShell. Join me for the next module, where we're going to go through designing our Azure SQL Data Warehouse.

Designing for Azure SQL Data Warehouse
What's in This Module?
Hi. I'm Warner Chaves from Pluralsight, and welcome to module 2 of the Azure SQL Data Warehouse: First Look course. This is Designing for Azure SQL Data Warehouse. In this module, we'll go through design choices that you have to consider for Azure SQL Data Warehouse. Even though the system is very similar to the SQL Server that you're used to, there are some design decisions that will likely be different. We're also going to go into the considerations on design for Fact Tables, we'll go through the considerations on design for Dimension Tables as well.

Distribution Methods and Keys
As you try out Azure SQL Data Warehouse, and you get further into the service, it's critical to realize that it doesn't have the same nature as SQL server like you're used to with on-premises. Azure SQL Data Warehouse will require different design decisions than if you are building an SMP SQL Server due to its different nature as a massively parallel processing system. Let's go through some of those design decisions in this module. The biggest difference from the symmetric multiprocessing nature of SQL Server, is the fact that Azure SQL Data Warehouse always has a distribution key for its tables. This distribution key will determine the method that Azure SQL Data Warehouse has used to express the data across multiple different nodes and storage. Every time that you create a table and you load data into Azure SQL Data Warehouse, it's going to split that table in 60 distributions. Think of that as 60 different buckets for data, and then those distributions will get attached to different nodes that will do some compute. For example, let's look at a Hash Distribution. Let's say we're distributing by the Product column. The first row arrives at Azure SQL Data Warehouse, the system will hash the value of Volleyball and put it in node 1. Second row will arrive to Azure SQL Data Warehouse with the same value of Volleyball for the Product column, it will get hashed to the same value, and also arrive at node 1. The third row, on the other hand, has a different value, it has a value of Basketball, which hashes to a different value, and then it gets assigned to node number 3. there is also round-robin distribution for records. In a round-robin distribution, the content of the records does not matter. The system will simply take each row and distribute them evenly amongst all the nodes of the service. So record number 1 will go to the distribution number 1, record number 2 will go to distribution number 2, and record number 3 will go to distribution number 3. When choosing a distribution method, the most important thing is to avoid a data skew on the distributions. A data skew scenario is one where the vast majority of the data is left in one distribution, and then we have other distributions like the green one that actually have a very small amount of data. This affects the performance of the system, because it's a massively parallel system and we want to have equal amounts of work for all the nodes to do. On an even distribution, all the nodes will have the same amount of data assigned to them, and they can all perform similar amounts of work when a query happens. This is what we want in our system, all of them doing work at the same time. So what makes a good hash key if we're going to pick a Hash distribution? First of all, the most important characteristic is that it distributes evenly. As we mentioned, that is critical so that we take full advantage of the compute power of Azure SQL Data Warehouse. Second, we want the hash key to be used for grouping, so that all the records that are going to be grouped are already on the same distribution. We also want the hash key to be used as join condition, especially amongst large fact tables, so that those same records that match on the join are in the same distribution, and we can avoid data movement. Fourth, the hash key column is not updatable. If you need to change it, you will have to delete it and reinsert the record, so it's better if it's just a column that is never going to be updated. And five, we mentioned that Azure SQL Data Warehouse will split a table into 60 different buckets called distributions, so we want a column that has at the very least, more than 60 distinct values, so that all distributions get some amount of data. If you think about it, you will easily realize that a round-robin distribution will always provide an evenly distributed system, however, it might not necessarily provide the best performance. This is because the data that has the same value, that could be used for the grouping or the joins, might end up on separate distributions and separate compute nodes.

Data Types
Let's talk about data types in Azure SQL Data Warehouse. We want to use the smallest data type which will support your data. This applies to integer values, it applies to character values, and it applies between choosing into ASCII or Unicode or varchar and nvarchar data types. If you have character columns, think about the default length that they have, and avoid defining them all to a very large default value, even if you're not using it. For example, if a column usually only uses 20 characters, don't define it to 100 characters just to be safe. As always, inspect your data and make sure that if you are choosing Unicode columns it's for a reason. Unicode will take twice as much space as a regular varchar column. The goal in choosing the smallest data types and the smallest default length possible for character types, is not only to save space, but also to move data as efficiently as possible between different compute nodes. This type of data movement does not happen on an on-premises SMP SQL Server, but Azure SQL Data Warehouse, being a massively parallel processing system, might have to do data movement to resolve a query. Keep in mind as well, that at this point in time, some complex data types, such as XML, geography, and others, are not supported on Azure SQL Data Warehouse yet. For the latest info, as we mentioned before, refer to azure. com.

Table Types and Partitioning
Let's go over the different table types that you can create inside your Azure SQL Data Warehouse. There are three main types. The first one is a clustered columnstore, which is meant for large tables, and is organized by columns. The second one is the Heap, which is meant for temporary staging tables or smaller permanent tables, and does not put any ordering in the data. And the third is the most common table type that we usually find in on-premises SQL Server, and that is the clustered B-Tree index, that is a table that is organized on a sorted column that usually becomes the clustering key. Let's go over clustered columnstore and when we should apply it. The clustered columnstore is the default table type that Azure SQL Data Warehouse is going to use if you don't specify anything else. Clustered columnstores provide a very high compression ratio because they organize the records by columns, so many columns have similar values, but they will be packed close to each other on the storage. Ideally, when we organize a clustered columnstore it is done in segments of 1 million rows, that means doing big inserts up to 1 million rows at a time, so that each segment is as packed as possible. This makes compression more efficient and processing more efficient. And finally, a clustered columnstore only has the columnstore itself as the main index. You cannot define any other secondary indexes. Let's talk about a heap. The heap provides no native index on the data, and no ordering on it. It is very fast for loading, because it doesn't have any other secondary structures pointing to the table. However, the heap provides no compression, and in large tables it will not give you the advantages that you get from a clustered columnstore. The only other advantage that the heap has is that it does allow secondary indexes on top of it for some very specific pinpoint-type queries. Finally, let's talk about the clustered B-Tree index. The clustered B-Tree index will presort the index based on that clustering key. That allows really fast singleton lookups when you are range scanning for a row in a small table, or when you're looking for a small amount of rows on a very specific range. A clustered B-Tree, however, will provide no compression, but it does have the advantage that, just like the heap, it allows for secondary indexes. Let's talk about table partitioning. Table partitioning is the feature where we take a top-level table, and under the covers it is split into different data buckets called partitions. This is a very common feature implemented on data warehouses. The reason why it's so common on SQL Server Data Warehouses is threefold. One, it's easier to load and remove data from a partition table by just targeting one partition. Two, you can target specific partitions to do maintenance, such as rebuilds or reorganizes. And three, you can also have some performance improvements on queries, because some partitions _____ get eliminated, depending on your query predicates. Now these advantages all sound very good, and you would think they would apply directly to Azure SQL Data Warehouse, however, it's not as simple as that. A highly granular partitioning scheme that will work with SQL Server, for example, a daily partition, can actually hurt performance in Azure SQL Data Warehouse. Let's see an example why. Let's assume we start with the 60 distributions. Like we said, that is the value that Azure SQL Data Warehouse is going to split a table into to distribute the data. After we have those 60 distributions, then let's assume we apply a highly granular partitioning scheme, like a daily partition for one year. That's 365 partitions. If we multiply those values, we realize that the table, in the end, would be split into 21900 data buckets. Now let's take that value of 21900 data buckets, and if we remember what we talked before, we realize that the ideal columnstore segment is 1 million rows. That means that in order to have an ideal setup for this type of high granularity partitioning scheme, we would need a table that has 21 billion 900 million rows. That is a very large table. For this reason, lower granularity partitioning schemes, like weekly or monthly, can actually perform better depending on how much data you have, because the ideal is to pack those segments of 1 million rows as much as possible.

Fact and Dimension Tables
So how do we apply these principles to a dimensional model, which is most likely what you would have in your data warehouse? For fact tables, large ones are better as Columnstores. Remember, we mentioned that the ideal columnstore segment size is 1 million, and we're going to have 60 distributions, so ideally, your fact table will be 60 million rows at the very least. Fact tables you will want to find through a distribution through a Hash key as much as possible, so they can be optimally joined and grouped. Remember, the most important part is that this distribution through a Hash key is even. Finally, fact tables should only be partitioned if the table is large enough to fill up each segment, and we should consider low granularity partitioning schemes, like weekly or monthly, for Azure SQL Data Warehouse. Let's talk about Dimension tables. Dimension tables can be Hash distributed or round-robin distributed if there's no clear candidate join key. Also, dimension tables are usually smaller, so round-robin might be a better fit. If you do have large dimensions, then a columnstore can be a good fit, but in this case it has to be a dimension that has multiple million records. The choice of heap or clustered index for a small dimension depends on what you're doing with the data. If you're not doing any particular sorted or range scanned operations that are looking for a small amount of records, then you can leave it as a heap. If you do have those operations, you can leave it as a clustered index and pick a proper clustering key. Both heaps and clustered index support secondary indexes, so if you have alternate join columns, you can just simply add those to your dimension table. Partition, in general, will not be recommended for dimension tables in Azure SQL Data Warehouse, because these are usually small tables, and will actually suffer by having a really small amount of rows per partition, per distribution.

Demo: Preparing the AdventureWorksDW Database
Let's check out a demo, and we're going to prepare the AdventureWorksDW database that we're going to keep using for the rest of the demos in this course. The first thing we're going to want to do is to download the AdventureWorksDW sample database from the internet. The address is msftdbprodsamples. codeplex. com. Once on this page, we can go to the downloads and scroll down until you find Adventure Works DW 2014 Full Database Backup, and download that. Once that is downloaded, we can go and create a RESTORE statement to restore it to our SQL Server. In this case I'm going to use a SQL 2016 as the target for my AdventureWorksDW. We can execute the RESTORE, and after a few minutes, the RESTORE will have completed, and we'll have our database. Once it's done, we can go in and verify that the database has been created by going into Object Explorer, refreshing our database list, and verifying that AdventureWorksDW is there. In the demo package, you're going to find a script called make_big_adventure. sql that we're going to run next. The purpose of this script is to increase the size of the Products and Transaction History table. In this case, we're going to use a script by SQL Server MVP, Adam Mechanic, to create a table called DimBigProduct and DimBigTransactionHistory. We can just run the script, and it will take care of creating many dummy product records and many dummy transaction records, so that we can properly analyze the tables and see if we are going to migrate them to Azure SQL Data Warehouse. After a few minutes, the script will have completed, and as we can see, now we have 30000 dimension products, and we have 37 millions of transaction history records that we'll be using for our subsequent demos.

Demo: Analyze Distribution and Data Types for Data Warehouse Tables
Let's check out a demo. We're going to analyze the distribution and the data types for Data Warehouse tables using the AdventureWorksDW database. I'm connected right now to the AdventureWorksDW database that we created in the previous demo. I'm using the file called AnalyzeDistributionAndTypes. sql that you will be able to find in the demo package. First, let's analyze the FactTransactionHistory table and see what it would look like if were distributed amongst 60 distributions round-robin. This query is going to read each record from the FactTransactionHistory, and then assign it from a distribution from 0-59. Let's see what the results look like. As we would expect, the round-robin distribution distributed all the records evenly amongst all the distributions that we were targeting. We can also see that, at most, one distribution to another is only off by one record. We know that round-robin is always going to distribute uniformly, however, it might not be the best thing for performance because records that have same columns that we could be using for grouping or joining, might end up on different distributions. So let's think about what would be the possibilities for Hash key distributions in the FactTransactionHistory table. First, let's look at the different columns in FactTransactionHistory. Column 1 is TransactionID. It is likely that it is unique per record. It could be used for joining, but it is unlikely that it would be used for grouping. ProductKey is likely to be used for grouping, and could be used for joining as well. ProductKey is likely to be repeated by many records as well. Quantity, on the other hand, is not going to be used for grouping, and it is not going to be used for joining. It's usually going to be used in the where clause. It's not a good candidate for the Hash key. ActualCost has the same characteristics as Quantity as well. OrderDate, while it could be used for grouping, is unlikely to be used for joining, and is more likely to be used in the WHERE clause as well, so it actually eliminates distributions instead of using the parallelism of the system. So far, ProductKey looks like it could be the best one for the Hash key that we're looking for. Now let's analyze the ProductKey. First thing we want to do is to make sure that the ProductKey has more than 60 distinct values, so that no distribution is left empty. We can see from the results there are 30300 different ProductKeys, so there's not going to be a distribution that would be empty, we have more than 60 distinct values. Now, with this query, we're going to see what the distribution could be if we loaded this data into Azure SQL Data Warehouse. What we're going to do is to group records by ProductKey, and then assign them a distribution from 0-59, like we did with the round-robin. We can explore the results, and we'll see the distributions go from half a million to over 700, 000. There is no distribution that is empty, and there is no distribution that is heavily skewed with most of the data, so, so far, the ProductKey looks like a good candidate. However, distribution is not everything, so we also have to think about query patterns, like I mentioned before. Now let's look at all the other tables in our model that use the ProductKey column. From the results, we can see that we have the DimBigProduct, the FactTransactionHistory, the DimProduct, FactAdditionalInternationalProductDescription, and other different fact tables that are most likely that we could join on the ProductKey. This makes the ProductKey the best candidate so far for a Hash key distribution of the FactTransactionHistory table. As part of our design analysis, we also want to take into account the data types. In this case we are going to run a query that will tell us what columns and what tables have Unicode values. Now here, from the results, we can see that we have some columns that are set to Unicode, even though the language is English. As we know, Unicode is for foreign language characters and special characters, and is not really necessary for just English names, so those columns could be changed to just regular varchar, instead of nvarchar. On the other hand, if we go through the results, we will find other columns, like ArabicDescription, ChineseDescription, and HebrewDescription. Looking at those, we know that those languages use a different character set, so those are okay to stay as Unicode, and we don't need to change them back. Once we identify a column that can be converted, we can run an alter table, alter column, and then change EnglishProductName from Unicode to a varchar so it's ready to be migrated to Azure SQL Data Warehouse. You could repeat the same analysis by checking the length of different character fields, and also checking the sizes of the different integer columns.

Module Summary
In this module, we learned that distributions are a new concept of Azure SQL Data Warehouse. This is a concept that does not have an equivalent in your on-premises SQL Server, but it is critical to understand it in order to design a proper warehouse in Azure SQL Data Warehouse. There are three table types, columnstores, heaps, clustered b-tree indexes, and we mentioned that are their strengths and what are their weaknesses. We also mentioned how partitioning is different in Azure SQL Data Warehouse, and it has to be analyzed carefully to avoid having very small, and possibly empty partitions. Then, we went in through the Fact and Dimension tables, and what are the best practices when figuring out how to place them into Azure SQL Data Warehouse. For the next module, join me as we're going to look into loading data into Azure SQL Data Warehouse.

Loading Azure SQL Data Warehouse
What's in This Module?
Hi. This is Warner Chaves with Pluralsight, and welcome to module 3 of the Azure SQL Data Warehouse: First Look course. This is Loading Azure SQL Data Warehouse. In this module, we're going to look at the best practices for loading data into Azure SQL Data Warehouse. We're also going to look at the different methods available for loading data and their different characteristics. And finally, we're going to take a look at the Azure Data Warehouse Migration utility, a utility that will help you plan your migration and execute it from your on-premises database to Azure SQL Data Warehouse.

Best Practices for Data Loads
The first thing to keep in mind when we discuss the best practices into loading Azure SQL Data Warehouse, is that it's an MPP system, massively parallel processing, so we want to do as much work in parallel as possible. Due to its parallel nature, Azure SQL Data Warehouse introduces the concept of Data Warehouse Readers. These are threads that will be reading data in parallel and then passing it off to Writer threads. As we can see from the table, as we scale Data Warehouse Units from let's say 100 units to 2000, our Readers also scale up from 8 Readers at Data Warehousing Unit 100, all the way to 60 readers at Data Warehousing Unit 2000. (waiting) So as you can see, your Data Warehousing Units have a direct impact on how fast you can load data in parallel. On-premises, it's not uncommon to be receiving inserts of data into a data warehouse from operational systems during the day, and we don't really think that much about the size of the batch that is inserting into the data warehouse. So we could be receiving a couple of records in a row, or maybe a batch of 10, or a batch of 100, but in Azure SQL Data Warehouse, for the largest tables, the optimized insert batch size should be as close to 1 million rows as we can get. The reason for this is that 1 million rows will compress a lot better, and will pack what is called the columnstore segment, just a storage unit of the columnstore indexes, which we discussed in the last module. Another good practice when loading into Azure SQL Data Warehouse, is to avoid ordered data. The reason for that is that many records in a row that are ordered on the Hash key will all end up in the same distribution, and then that distribution will be basically a bottleneck to how fast we can load the data. So it can be a little bit counterintuitive, but what we want actually is to not have the data ordered, so that we don't produce hot spots that slow down the load operation overall. Using temporary tables is also a good practice, especially if it's data that we're still going to _____ massage further. For example, we have some raw data in a CSV file, and then we're going to load it into a temporary table, which is a regular table, you just have to specify the pound sign before the table name, and then after we've done some transformation, we actually finally load it into a distribution. Staging and transforming into a temp table that's a heap table is recommended as the fastest way to manipulate the data before moving it to its final permanent storage in the Azure SQL Data Warehouse distributions. CREATE TABLE AS is a very useful construct in Azure SQL Data Warehouse. It can help change different facets of a table, like its distribution or its partitioning, based on another SELECT query. This can be really fast, and also performs with minimal login. In the example at the top, we can see that we're just creating a temporary table, and we're using the round-robin distribution, and we're just taking the content of FactInternetSales. Now let's assume, for example, that the reason we're doing that is because we need to refactor that table. So we're going to load it into a temporary table, then we're going to modify it either through updates or inserts or deletes, and then finally we can do another CREATE TABLE AS to replace the original table. As I said, CREATE TABLE AS performs really well. It's a fully parallel operation that takes full advantage of all the different nodes in Azure SQL Data Warehouse. It is minimally logged, so it works a lot faster than fully logged operations that write every single record to the transaction log, and it can help you change the distribution or the table type or any partitioning scheme.

User Resource Class
A very important concept in understanding how to manage resources and loading data in Azure SQL Data Warehouse is the user resource class. These are basically database roles that govern how many resources are given to a query, and that includes queries that load data. There are four resource classes that a user can belong to in Azure SQL Data Warehouse. The first class is called Smallrc, and this is the default class that will get assigned to a user if no other resource class has been assigned, and it always has the same amount of memory assigned for a small resource class query. Even if you increase the Data Warehousing Units, it's always 100MB per distribution. On a medium resource class, the amount of memory fluctuates between 100MB-1600MB. On a large class, it fluctuates from 200MB-3200MB, and on extra-large it can fluctuate from 400MB-6400MB. The lower range that is shown on the table corresponds to what you get in the Data Warehousing Unit of 100, and the upper range corresponds to what you get in a Data Warehousing Unit of 2000. For fast and high quality loads, it's recommended to create a user just for loading, and don't leave it in the default small resource class, use a medium or large resource class instead.

Loading Methods
Let's talk about the different loading methods for Azure SQL Data Warehouse. The first category is single-client loading methods. This includes using tools like SSIS, Azure Data Factory, or BCP, the Bulk Copy Process utility. These methods can add some parallel capabilities, like for example running multiple BCP imports at a time, but ultimately they are bottlenecked at the Control node. And we're going to talk about the Control node in the next slide. The second category of methods is parallel reader loading methods. This is done through a module called PolyBase. PolyBase exists in the SQL Server 2016 on-premises version. It also exists in the appliance that Microsoft sells called Parallel Data Warehouse, and as such, it also exists inside Azure SQL Data Warehouse. What PolyBase does is that it reads from Azure Blob Storage either flat, compressed or uncompressed files, and can lower the contents in parallel into Azure SQL Data Warehouse. It bypasses the Control Node and loads the data directly into the Compute Nodes and the distributions. So let's talk about the concept of the Control Node in Azure SQL Data Warehouse. Control Node is basically a special category of node that receives all the connections and orchestrates the queries. It'll send the different queries to the different nodes and take the results back, so it's always in constant communication to the different computes nodes that are a part of the system. The Compute Nodes, on the other hand, read data off of the distribution, do all the processing, and scale with the Data Warehousing Units. This is something that is important to understand. The Control Node will always be the same, regardless of how many Data Warehousing Units you add, however, you will get more compute nodes as your DWUs increase. So let's see what happens when we load through SSIS. For example, we can have a large CSV file with many values, and then that CSV file is going to be read by the SSIS service, and then from SSIS it's going to connect into the control node, and then from the control node it will go into all of the different distributions of Azure SQL Data Warehouse. So as we can see, the control node here can be a bottleneck for the system. What we want to do is only load small amounts of data with SSIS.

Demo: Loading with SSIS
Let's take a look at a demo, and see how we can use SSIS to load data into Azure SQL Data Warehouse. Okay, I'm connected right now to the data warehouse, and the first thing we're going to do is to create the table that we're going to import with SSIS. If you notice, at the top I'm going to create a schema called prod, just for easier management of tables. For the rest, we just have the DimBigProduct table that we're going to import, and it has the exact same schema as the table that we are going to have on-premises with one small difference. As we can see, at the bottom I'm going to specify that I want this to be a clustered index table on the ProductKey. Note, I'm doing this because the dimension table is not that big, so it won't benefit from a full columnstore index. Once we're on SSIS, we're going to see that we have two Connection Managers. The first one is going to connect to my local SQL Server 2016 AdventureWorksDW database, and then I have another Connection Manager. This one connects to my server that has my data warehouses in Azure. I'm going to use my warneradmin account, and I'm going to connect to the warnerwarehouse database. For the package, I just have a simple data flow called Gated Load Through Control node, because we know SSIS will just put all the data through the control node. It's composed of a local SQL Server source, where we're just going to specify the DimBigProduct as the table source, and then on the destination it's another OLE DB destination that is connecting to the data warehouse. And, again, we just specify the prod. DimBigProduct table that we created with the script at the very beginning. At this point, we can just start the package. The system will connect to my warehouse in Azure and start moving the records. As we can see here, after a few seconds, the whole transfer of data is completed. To verify the results, I can go back to Management Studio and then do a simple SELECT * from prod. DimBigProduct, and we can verify that we get the exact same amount of rows that we loaded through SSIS, 30300 rows.

PolyBase
Let's talk about loading data with PolyBase, which is the recommended method for large amounts of data so that it happens in parallel. When we load with PolyBase, let's say we have a large CSV file that is in Azure Blob Storage, it actually gets read through different threads directly into the compute nodes that handle the distributions of Azure SQL Data Warehouse. In this type of scenario, the control node is not involved at all through the load, and so it scales a lot better and the loads happen a lot faster. PolyBase can load data from UTF-8 delimited text files, it can be pipe, comma, or tab delimited text files, and also popular Hadoop file formats like RC, ORC, and Parquet. You can also compress the files before you load them using gzip and zlib, or the snappy files. Something to keep in mind is that if you are using a compressed file, PolyBase will not be able to fire multiple readers against the file, because it needs to decompress it before it can understand it. Multiple readers will only work against an uncompressed file. Let's take a look at the different steps for setting up PolyBase. First, we need to create a master key, because that key is going to be used to encrypt credentials. Second, we're going to create a database scoped credentials with the storage key that points to our Azure Blob Storage. Third, we're going to create an external data source, basically telling PolyBase to use that credential to connect to a specific path inside Azure Blob Storage. Fourth, we're going to create an external file format. This is where we tell PolyBase the type of delimitation or format that we are expecting from our files. Fifth, we create an external table. This is how we tell PolyBase the amount and type of the different fields in the file, and how they map to an actual tabular structure. Once we have the external table, we can finally load from it and bring the data in into Azure SQL Data Warehouse.

Demo: Loading with PolyBase
Let's do a demo of loading data with PolyBase. Okay, I'm connected right now to my SQL Server 2016 on-premises, and the first thing we're going to do is to export the data that we are going to load into PolyBase. So we'll go into Databases, and then right-click on the AdventureWorksDW database, Tasks, and then we're going to go into Export Data. As the data source, we are going to select SQL Server Native Client, and we're going to select our local server and AdventureWorksDW Database. We'll click Next, and then it's time to select the destination. In this case, we're going to select a Flat File Destination since PolyBase works with flat files. We'll go to Browse, I'm just going to go to C, Temp, and I've created a folder called FactTransactionHistory, since that's the table that we're going to export, and I'm just going to name the file the same. I will set the format to Delimited, I don't need a Text qualifier in this case, and then finally I'm going to omit the column names in the first data row, since we don't need them in this case. Then we'll go with Next. We're going to say we're going to copy data from one table or view, and then we're going to select the source table to be our FactTransactionHistory table. Then click next, and then we can run the export immediately, verify all the settings at the end to make sure that we have the right source and destination, and then we'll click Finish. At this point, the export will kick off, and after a few minutes the whole 30+ million rows we'll have done. As we can see, the file has been exported, and it's about 1. 6 GB in size. Now to get the file into Azure storage, I recommend Microsoft utility Azure Storage Explorer, which you can get from storageexplorer. com. This utility will allow you to easily connect to your storage account and upload the file. Let's see how. I already installed it and configured it, and as you can see, right now I'm connected to a storage container called datawarehouse. I can just go to Upload, and I'll select to upload a folder. I'm going to browse through the folder that we created earlier that has that FactTransactionHistory file, select it, and then make sure that I'm using the Upload to folder so that it also replicates that folder name into the storage container. Click Upload, and then the utility will start uploading the 1. 6 GB from this on-premises machine over to the Azure storage account. After a few minutes, it will complete, and as we can see it has created the folder inside our datawarehouse storage container, and if we go into that, we can also see that now we have our 1. 6. txt file inside that folder. Now let's go over the different steps to set up PolyBase on our Azure SQL Data Warehouse. We need to create that master key if we haven't done that already, that's step number one. Number two, we're going to create a DATABASE SCOPED CREDENTIAL that will have the storage key from Azure storage account. The identity doesn't really matter, we'll just put user' in this case, what matters is the SECRET, which is the storage key that you can get from the Azure portal for your storage account. I'll show you where. You just find your storage account, click on it, and then on the panel on the right side, there's the Access keys section. On that section, you will be able to find the access key that you need to use on your PolyBase setup. Going back to our script, the third thing we're going to do is to create the external data source. In this case, we're going to point to that external data source to that storage account that we created before, and the one that we just saw in the portal. This is the format. We put the datawarehouse as the container @dataimports. blob. core. windows. net. And then we also specify the credential we're going to use, which is the one that we created in the previous step. Fourth, we exported the table as a comma-separated value file, so we're going to create that file format, CSVFileFormat. We're specifying the FIELD_TERMINATOR to be the comma, no STRING_DELIMITER, the DATE_FORMAT that we used for our data, and USE_TYPE_DEFAULT = FALSE. This means that we're going to have null instead of using a default value, if the row has no corresponding value on the text file. Now I have created a schema called stage. I recommend to do that for non-permanent tables, and for tables that you're just going to use as your staging area inside of the warehouse. In this case, I'm creating the external table on that stage schema. This EXTERNAL TABLE has the same schema that we're expecting from the file, so we say the location, which is the folder, the DATA_SOURCE, which is the AzureWarehouseStorage we created, the FILE_FORMAT, which is CSV, REJECT_TYPE we'll set to = VALUE, and the VALUE we'll set it to 0. That just means that if more than 0 errors happen, so just 1, the entire load is going to fail. Finally, we can use the CREATE TABLE AS structure to bring the data with PolyBase. In this case I already have a schema called prod, but we can create it if we don't have it, and then we create the table with DISTRIBUTION = HASH on the ProductKey, and then just a SELECT * FROM the external table. And I'm going to use a label here so I can easily identify the operation as a load. At this point, I'm going to execute the CREATE TABLE AS statement, which is the one that's actually going to load the data. And then I'm going to use this query that is in the MonitoringTheLoad SQL file, to look at all the different threads that are doing work to load this file. As we can see, we have multiple writers that are BulkInsertProcessing the data, 60 writers, like we discussed before, and in this case I have 8 EXTERNAL_READERS, which is the amount of readers that I will get with the Data Warehousing Unit value of 100. Then we also see we have other workers called HASH_CONVERTERS that are the ones that are processing the data inside each distribution to assign the Hash value to each one of those keys. After the load is completed, you'll be able to see how the status has changed. And then we want to verify that we loaded all the data that we wanted correctly. And then we're going to run this command, DBCC PDW_SHOWSPACEUSED, and pass in the value of the table that we're interested in. Here it's prod. FactTransactionHistory, which is the table that we just loaded. We can select it and execute, and look at the results, and they will tell us how many rows have been allocated to each distribution. So we can see the result set has 60 rows, one record for each distribution, and we can see that around 600, 000, 500, 000 or so have been allocated to each distribution, just as we expected.

Azure SQL DW Migration Utility
To support migration efforts, the Azure SQL Data Warehouse team is also working on a migration utility that is right now in preview, and they continue to improve month after month. This migration utility supports SQL Server 2012 and up as a source, and Azure SQL Database. It also provides a migration report pointing out possible issues that you might have with the schema, or that you need to take care of before migrating to Azure SQL Data Warehouse. It also assists with the schema migration, giving you scripts that you can apply in Azure SQL Data Warehouse, and it can also assist with data migration, creating scripts that use BCP to do export and import of your data into Azure SQL Data Warehouse.

Demo: Using the Migration Utility
Let's do a demo and take a look at using the Azure SQL Data Warehouse migration utility. First, let's look at where we can find the Azure SQL Data Warehouse migration utility on the Azure website. We'll go to azure. com, and from the front page we're going to select Documentation. From here, we're going to select Data & Storage, and at this point select SQL Data Warehouse. On the left panel, we're going to select Migrate, and then we can select Migration Utility. On that page, we can just click the button that says Download Migration Utility. That will download a zip file, and from the contents of the zip file we can install the migration utility. Once installed, we can double-click on it, and the first thing we're going to check out is that we have two different source types, we have SQL Server and Azure SQL Database, in case you want to migrate from a SQL Database that is already in the cloud. And the destination, only one, Azure SQL Data Warehouse. At this point, we can click Next, and then we can connect to our source. In this case, it's my local Server Name, so I'm going to specify my SQL 2016 instance that I have, and I'm using it as a source. Connect to it, and it'll read it, and it'll identify that the name is AdventureWorksDW, and the database size is just 3. 26 GB. Now the first thing we're going to do is check out the compatibility, which is one of the options at the top. We'll click on it. Azure will do the analysis, and then we can save that as an XL file, and then ask if we want to open it, and we'll just say Yes. Once we have it open, we can see the different types of observations that the utility did about our database. We can see things like the issue of some primary key constraints on tables, since Azure SQL DW does not support primary keys. We can see that there are some issues on some unique NONCLUSTERED indexes, because uniqueness is not also enforced by Azure SQL DW, and so on. I recommend you go through the list of the different items, and determine where the change has to happen in your database, and whether you need to do some extra work to deal with the constraints that don't happen in Azure SQL Data Warehouse. And same, you can see that down the line there's also analysis type of code, and that just basically means your stored procedures, functions or triggers might have some code that is not compatible yet with Azure SQL Data Warehouse. Now in the meantime, this is still a preview utility, so you can see that it gives you observation, but doesn't exactly tell you where or which object is the actual offender. Hopefully this will come in a subsequent release. Back in the migration utility, we're going to select the AdventureWorksDW database, and then we're going to click on the Migrate Selected button. This is going to load a list of all the tables in this source database. At the very end, there's going to be three different options related to how they get migrated to Azure SQL Data Warehouse. We can pick the Table Type, the Distributed column, and the percentage of data to move. Table Type is either Round Robin or Distributed, like we saw in previous modules. In this case, we're going to pick Distributed, and here we can select the Distributed column. We're going to leave it as ProductKey in this case, and we're also going to select the 100% of data to move. We're going to select the table, in this case FactProductInventory, and then we're going to go to Migrate Schema. As we can see, the tool has created the script to create the table, and it has populated the options that are specific to Azure SQL Data Warehouse, like specifying that it's the CLUSTERED COLUMNSTORE INDEX, that the DISTRIBUTION is a HASH distribution, and that it should be done on the ProductKey. Now we need to do a small modification here and replace the dbo schema with the prod schema, which is the one that we are using in our Azure SQL Data Warehouse. We can run the script, and then we'll have to populate the settings to connect to our Azure SQL Data Warehouse. So I specify the Database, the Server Name, and then I specify my User Name, which I'm just going to use my admin account, and the password for it. When all that is done just click OK, and the utility will connect and apply the script. In this case it's already successful. After the script has been applied, then we can migrate the data. In this case we're going to select BCP PACKAGES, the SSIS packages have not been implemented yet, and in the BCP options we can change the row or column delimiters if we want. We're ready to do the migration. We can just pick a path where the BCP scripts are going to get written. In this case I'm just going to pick a new path called AdventureWorksDWMigration, and then click next. And then I'll just generate it. Generation completes, and then we can actually go and browse into the File Explorer and find the scripts generated by the utility. In this case we see it has generated two different scripts, an Export and an Import script. We'll look at the contents of both. When we open the Export script, we can see it's basically a select that's going to be using BCP to export the table from our SQL 2016 AdventureWorksDW off into a flat file. Now let's check out the Import. We'll open it up, and the first thing we need to do here is, again, to fix the dbo and replace it with prod, since that is the schema that we're using. After that, we can see that it's just basically doing a BCP import from the flat file into my Azure SQL Data Warehouse database. We can close that, and then save the results. At this point, we're ready to run both scripts through the command line. So we can open the command line, and then we'll just browse through the location of the scripts. We can verify we can see both scripts, and then we can just run the export first so that we generate that flat file. And then after the export is complete, we can run the import so the flat file contents get uploaded to Azure SQL Data Warehouse. As we can see, we can follow as the progress goes along, and it's done. Now at this point, what we can do is go into Management Studio and properly test that the table was created and populated inside Azure SQL Data Warehouse. So we're simply going to run a select * from prod. FactProductInventory. If we execute that, we can wait for it to finish, and then at the end we can confirm that the same number of records were imported into Azure SQL Data Warehouse and the migration of the table is complete.

Module Summary
So far in this module, we learned that there are several best practices for loading data that can help you get the best loading times into Azure SQL Data Warehouse. We also learned there are two different node types. There is the Control node and Compute nodes, and they have different roles inside the system. We also learned that there are two different types of load methods, single-client and fully parallel loads that are done through PolyBase, which is the recommended way for large amounts of data. Finally, we also showed how to use the Azure Data Warehouse migration utility to plan and test your migration. Join me on the next module where we're going to look at Querying and Tuning Azure SQL Data Warehouse.

Querying and Tuning Azure SQL Data Warehouse
What's in This Module?
Hi. This is Warner Chaves with Pluralsight. Welcome to module 4 of the Azure SQL Data Warehouse: First Look course. This is Querying and Tuning Azure SQL Data Warehouse. In this module, we're going to go over the concurrency and transaction model of Azure SQL Data Warehouse, which is quite different from the on-premises SQL Server. Then we're going to go over maintenance operations that are still your responsibility as a user of the service. And finally, we're going to take a look at how we can track and scale query performance to get the biggest benefits out of our data warehouse.

The Concurrency Model
In order to understand querying and tuning Azure SQL Data Warehouse, we need to understand how Azure SQL Data Warehouse manages its workload. Let's think about the actors that come into the service. First, we have users, and they belong to a resource class like we've seen in previous modules, and they submit queries to the service. On the other side, Azure needs to execute these queries, while at the same time making sure that the service provides predictable performance and controlling the resources of the system. This is done through the three elements of workload management. First, we have the user resource class, which we discussed in previous modules. Second we have the concurrency model, which we'll go over in this module. And finally, we have the limits on specific transaction sizes, which we'll also review during this module. So what is the concurrency model? Basically this is a system that Azure implements to control how many queries can execute at any given moment inside the data warehouse. There are two maximum limits that apply to the concurrency model. There is a limit of 1024 connections, and then there is also a limit of up to 32 concurrent queries. However, a lot of the times, based on the Data Warehousing Units, you won't even be able to reach the 32 concurrent queries, and we'll go into details on why that is. As we can see from the table, the number of concurrency slots increases as we increase the number of Data Warehousing Units. If there are more than 32 concurrent queries, or you have exceeded the number of concurrency slots, then the query is going to be queued until both of these thresholds are satisfied. So it's very likely, for example, that in a Data Warehousing Unit of 100 you will just hit the limit of 4 concurrency slots before you ever approach the maximum system-wide limit of 32 concurrent queries. As a matter of fact, if you look at the table, only at the level of Data Warehousing Units of 1, 000 do you have enough concurrency slots to be able to go to 32 concurrent queries. For now, let's assume each query is consuming 1 concurrency slot, and analyze _____ scenario. Let's say we have a data warehouse of 200, and at that same time we have 7 queries executing. Now we have 2 queries incoming, and keep in mind, DW200 has an 8 concurrency slot limit, so what happens is that 1 query gets queued. On the other hand, if we have a DW1000 and we have 32 queries executing, and then we have 2 queries incoming, then we're going to have 2 queries queued. In the first case, we're hitting the limit of the concurrency slots, and in the second case we're hitting the maximum limit of the 32 concurrent queries. Both limits always have to be satisfied before a query gets executed, otherwise, it will get queued. The concurrency model is actually a little bit more complex, because there's a different amount of concurrency slots that are given to a query depending on the user resource class. As we can see from the table, the Smallrc resource class will always get 1 concurrency slot, regardless of the amount of Data Warehousing Units. However, for the medium, large or extra-large resource classes, the amount of slots fluctuates as the Data Warehousing Units increase. So let's look at an example of how the resource class and the concurrency slots interact to determine the overall amount of queries that you can run in the system. Let's say we're using a DWU200 system, and we're going to use the small resource class, so smallrc. The smallrc takes few concurrency slots. We know that it takes 1 slot per query, and the DWU200 system can take 8 queries at a time because it has 8 concurrency slots. Now let's look at what happens if we select a larger resource class. The same example on a DWU200 system. Let's say we pick a large resource class, that's the largerc. And then this type of class takes more concurrency slots. The bigger the class, the more concurrency slots it takes. So 1 largerc query actually takes 4 slots per query. Since we know that the total amount of slots in a DWU200 system is 8, then we get less concurrent queries overall, only 2 largerc queries can be running at the same time. Let's see a more complex workload example where both the user resource class and the concurrency slots come into play. We'll start with a data warehouse of 1000 units. This gives us 40 concurrency slots, and remember, there's always the maximum limit of 32 concurrent queries. Now, we're going to run 20 smallrc queries, and we know that a smallrc query consumes 1 concurrency slot, so this is 20 concurrency slots consumed at this point. Then we're going to run a mediumrc query. In a data warehouse of 1000 units, that type of query consumes 8 slots, so we get 21 queries consuming 28 concurrency slots. In this state of the system, the following could happen: We could execute 11 more smallrc queries, and that would bring us to 32 queries total running in the system using 39 concurrency slots. Or we could run one more mediumrc query and that would bring us to 22 concurrent queries in the system running on 36 concurrency slots. Or if we wanted to run any query of a higher class, like largerc or xlargerc, it would just get queued, because it would go over the 40 concurrency slot limit of a data warehouse of 1000 units. The concurrency and resource class interactional limits only applies to reports and user database queries. It does not apply against queries that are analyzing in system views, creating stats, or other management commands. These don't use concurrency slots.

Transaction Sizes
The other element of workload management is the transaction size. This controls how much data can be part of our transaction inside the data warehouse. As we can see from the table, the transaction limits also increase as we increase the Data Warehousing Units, and these limits are specified by GB per distribution. So at a DWU level of 100, we actually get 60 GB total because we get 1 GB per distribution. So for example, if we look at a DW200 level, if every distribution is doing equal work, then each one could consume up to 60 times 1. 5GB, or that is 90GB total of space for one transaction. Something that you have to keep in mind, though, is that a heavily-skewed distribution can make a transaction hit the limit before it reaches the entire system-wide transaction limit. For example, in a DW100 system the per distribution limit is 1GB, so if one distribution has to consume more than 1GB, it will fail, even if the other distributions still haven't reached their 1GB limit.

Demo: Monitoring Concurrency
Let's check out a demo of how to monitor concurrency and queueing levels in the system. I'm connected again to Azure SQL Data Warehouse, and we're just going to run a simple sum of a UnitsBalance from a table of FactProductInventory. I have this GO statement with 10000 so that it will simply send that same sum 10000 times to the server for processing. That's how we're going to simulate load on the server, and it'll start executing. Now to simulate concurrency, I'm going to execute the exact same statement on four other windows. So that's window number 2, and that's window number 3, this is window number 4, and window number 5. My Data Warehouse is 100 units, so I should only be able to execute four queries at any given time. I'm going to use this monitoring query, which is in the QueueMonitoring. sql file, which you can find in the demo package, and this query is going to help us identify if there are any requests that are currently suspended and waiting on a resource, such as in the queue. I'll execute it, and we can see that we get one request with a Wait_type of UserConcurrencyResourceType. This is the Wait_type that identifies when a query has been put in the queue because we've gone over the concurrency limits. We can also verify here on the state column how the query status says Queued. We can also scroll further, see what type of command that is running, and also see the resource class of the request, which is very useful if you need to troubleshoot why a query is getting queued by the system.

Maintenance Operations
Let's talk about what type of maintenance is necessary in Azure SQL Data Warehouse. There are three types of stats that live inside Azure SQL Data Warehouse. There are single-column stats, and these are exactly what they sound like, just statistics in a single column. There are multi-column stats, which make a histogram of the first column and then it stores the relationship with the other columns in the Stats object. And finally, there are the Index Stats that get attached to every index that gets created inside Azure SQL Data Warehouse. Now a big difference between Azure SQL Data Warehouse and the on-premises SQL Server is that at this point the service is not automatically creating or maintaining the stats, it's up to you to do it. Let's check out recommendations for creating new stats and updating existing stats. First, a good start is to create sample single-column stats on the most important columns in the tables. Multi-column stats are recommended for joins that involve multiple columns. To choose the best candidates to create stats on, focus on columns that are used in JOINs, GROUP BY, HAVING AND WHERE clauses. If necessary, and you don't think you're getting accurate cardinality estimations, you can try increasing the sample size all the way to a full scan, though keep in mind a full scan is likely to take a long time to complete on a very large table. For updating existing stats, if there are new dates or dimension categories that have been added to the warehouse, then you should update the stats for those tables. If new data loads have completed, you should update the stats for those tables as well. If an UPDATE changes the distribution of the data, or if a DELETE changes the distribution of the data, you want those stats to reflect those changes in their histograms as well. Index defragmentation can still play a part in Azure SQL Data Warehouse. A heap, in this case you don't have to worry about, and doesn't have a defrag option. A B-Tree index can still use the index defragmentation, and is useful for removing low levels of fragmentation. A columnstore can do a defrag operation by proactively compressing CLOSED rowgroups. If you're doing large one million inserts, then this is unlikely to be useful, however, if you did a lot of small trickle-type inserts, you might want to consider running a defrag after the operations are done. The other index maintenance that you can do is a full index rebuild. On a heap, an index rebuild is used to remove forward pointers so access to the data becomes faster. On a B-Tree index, an index rebuild is recommended if you have high levels of fragmentation. And in a columnstore, an index rebuild can increase the density of the segments, because Azure SQL Data Warehouse will rebuild the entire columnstore and merge rowgroups so that they are as close as possible to the one million records of density. Keep in mind, if you do get to the point where you need to rebuild an index, that is an offline operation in Azure SQL Data Warehouse. As you're doing your maintenance, if you do find a large table that has heavy fragmentation, it is often faster to recreate the table with the CREATE TABLE AS construct that we saw in a previous module, and then switch it with the older one instead of doing a full index rebuild.

Scaling Performance and Query Labels
Let's talk about scaling performance of the system. In this case, we're going to focus on two different operations that you can do to scale performance. One is to increase the resource class assigned to the query, and two is to increase the Data Warehouse Units that are assigned to the entire system. Increasing the resource class is done by simply assigning a larger resource class as a role to the user. For example, in this case we're just using the stored procedure, sp_addrolemember, and we're going to add the loaduser' into the largerc' class. A higher resource class will get assigned more memory and CPU, however, the more concurrency slots that you use through a higher resource class, the less concurrent queries that you can run into the system. The highest role assigned will take precedence, so keep that in mind when you assign a user to multiple different resource classes. Increasing the DWUs is pretty easy as well. You just simply have to use this ALTER DATABASE statement and modify the SERVICE_OBJECTIVE to the new level that you want your DWUs to be. DWU increase is an offline operation. You might need to do a disconnect and reconnect while the change happens. If the client has retry logic, it might not even notice that there was a change in the connectivity. It is a good practice to make sure that there are no loads or transactions in progress, because when the change of DWUs happens, all transactions that are in-flight will be canceled and rolled back, and this could take a long time if you've been running a very long transaction. The change in Data Warehousing Units is not only possible through T-SQL, like we show here on this slide, but it can also be done graphically through the Azure Portal. Finally, let's talk about tracking queries with a very useful functionality called Labels. Let's say we have a user, and it's submitting a query to the system. They can tag it with a label that can then be used by the administrator to run diagnostics on the system and identify the particular query with that same label. For example, let's assume we have a user query and we're just going to run a sum of the quantities from our transaction history. By specifying the OPTION (LABEL = QuantitySum'), the administrator can run a tracking query that uses the dm_pdw_exec_requests dynamic management view to only see the status of the request for this particular query by using a WHERE clause that is looking for that label of QuantitySum'.

Demo: Scaling System Performance
Let's check out a demo now of scaling up the system for performance. I'm connected right now, again, to the Azure SQL Data Warehouse, and we're just going to run a query that is going to do a couple of joins on the FactTransactionHistory table, and then calculate some aggregations. The whole point of this query is just to have it run for a significant amount of time so that we can tell the difference between one level of data warehousing Units, which right now is 100, and then increasing to 200 Data Warehousing Units. We'll select it and execute it, and this particular query we'll start returning some results and finish in about 12 seconds, returning a little bit over 130, 000 rows. So let's say we want to scale up this query, and we want to do it by increasing the amount of DWUs that are assigned to our warehouse. First, we'll run the first query in this file called ScaleUpAzureDW, which you will find in the demo pack, and the first query is going to show us what is the current amount of DWUs assigned to our warehouse? As we can see from the results, right now my DataWarehouse is at DW100 level. So if we want it to scale up, we could go to 200-400, in this case we're just going to go to 200. And we just have to run an ALTER DATABASE MODIFY SERVICE_OBJECTIVE command. We'll select it and execute it, and then it finishes successfully. Now at this point we can go back and rerun our query, which remember, was taking about 12 seconds. We'll just execute it again, Management Studio will take over it, and now it finishes in 8 seconds, returning the exact same amount of rows. If we run it again, this time, because of caching in the system, it actually runs again in only 4 seconds, so we can see some good benefits right away just by increasing the amount of DWUs. If we wanted to go back because we are happy with the results we got and we don't need that power anymore, we can immediately just modify the ALTER DATABASE statement and just bring it back down to the DW100 level. This is the nice thing about Azure SQL Data Warehouse, it can not only scale up, but you can also scale down when you no longer need the power. We'll just select it again and execute it. We can also verify what's going on in the portal. So if I go to the portal now, and I'm going to go into SQL Server and find my warnerdatawarehouses server, navigate to that one, and then navigate to my warnerwarehouse, we can see the status right now is set to Scaling. If I select it, then we get that message at the top that says Scaling, and it's updating the performance from DW200 to DW100. After a few minutes, the system will be done with the scaling operation, and we're back to the level of 100. If we don't even need the DW100 because nobody is using it, we can also pause the warehouse. Azure will receive the Pause request and will start to turn down the system. After a few minutes, we'll get notification that the database has been paused, and we can go and see the detail, and we can see it took approximately 4 minutes since we submitted the request and the database is paused, and at that point we're no longer paying for compute, and we're only going to be paying for the storage, which is great if you don't have a workload that's 24/7 and you have times when you can pause the compute.

Demo: Labeling and Tracking a Query
Let's check out a demo of labeling a query and tracking its execution. I'm connected to the data warehouse, and we're just going to run a query that does some aggregations of some transaction information for some given products. The whole point here is to look at the OPTION (LABEL = Query: Products and History'). This OPTION will allow us to track the execution of the query with different system views, all tracked by the LABEL. This type of functionality makes it easier for you to identify specific queries or reports that are going into the system, and to track their performance down. So let's select the query and execute it, and it should complete in a few seconds. And now, after the query has run, we can use the system views to track the execution of this query through through the LABEL and see the type of work that it did. Let's check it out. These system queries can be found on the MonitoringLabeledQuery. sql file in the demo pack, and we're going to be looking at three different levels of monitoring. One is going to be using the sys. dm_pdw_exec_requets DMV to monitor at the request level. Second, we're going to use the dm_pdw_dms_workers view to monitor at the worker level. And then finally, we're going to be using dm_pdw_sql_requests to monitor at the distribution level. First, let's run the query to monitor the request level. As we see here, we only get one result back at the request level. This is because requests map one-to-one to a query. We can get some important information, such as the runtime, and then we can see, for example, the resource class and the command that had that specific label attached to it. We can go back, and this time we're going to run the query that does the monitoring at the worker level. So this will map each request, so each query, to the multiple workers that get assigned to it inside Azure SQL Data Warehouse. A worker in this case, is pretty much a thread that is running and doing some work inside a specific distribution. To get this information, we're going to use the exec_requests and dms_workers views from the system. Let's see what the results are. So when we run this type of monitoring, in this case we get 120 rows. The reason for this is because we had two threats that did some work, and we have 60 distributions in Azure SQL Data Warehouse, so that comes to a total of 120. We see that the first type of operation that happened was HASH_READER threads. After that, what we had were 60 WRITER threads also doing work inside Azure SQL Data Warehouse for this particular query. The view also provides us with some information that is very useful for troubleshooting and tuning, like the bytes and rows processed, which would allow us to find if there is a particular distribution that is overloaded or one that is not doing enough work. We can also see the elapsed time per worker, and the cpu_time as well. If we go back to the Editor, then we can look at the final query, which uses the exec_requests view and the sql_requests view of the system to see what the operations were, but this time we're not going to look at it from the worker point of view, we're going to look at it from the distribution point of view. So we're going to get a record for every operation that happened inside a distribution. Let's look at the results. We'll select it and execute it, and then we get 180 records returned. If we think about how Azure has 60 distributions, then we can see that each distribution had three different operations that happened inside of it. If we scroll over, we can see that we get start and end time, and the total elapsed time per operation, so we could identify long operations happening inside of distribution. And the command actually gives us a glimpse inside of what Azure is doing to resolve the particular query. In this case we can see that the first operation was to create a temporary table inside each distribution, and this temporary table was being populated with the values that we wanted from the DimBigProductTable, like ProductKey and the EnglishProductName. If we scroll down, we can see the other operations that happen inside each distribution. For example, we see the second operation was the actual SELECT statement that resolved the records to return from the query from each distribution. Finally, if we keep scrolling down, we can see that the last operation on the distribution was to actually do some cleanup, to drop the temp table, and then to select the amount of records that were returned by the query. I hope you got a good overview of how useful labels can be, so that you can track the execution of the query at these three different levels: request, workers, and distribution.

Module Summary
In this module, we learned about Azure SQL Data Warehouse's Concurrency Model, how it's very different from SQL Server and it depends on concepts like resource classes and concurrency slots. We learned that the service imposes a limit on transaction size as well, and this limit increases as the Data Warehousing Units increase. We also learned that there are several maintenance operations that are still our responsibility, such as creating and maintaining stats and index maintenance. We learned that the DWUs can be scaled elastically up and down depending on the workload, and we can even pause compute completely if we're not using the data warehouse. And finally, we learned the concept of query labels and how they can help identify a query and help troubleshoot it faster by the user of the label. So here we come to the end of the course. I hope you got a really good overview of the service, and I urge you to get out there and give Azure SQL Data Warehouse a try. Until next time, thanks for watching.